# -*- coding: utf-8 -*-
"""Fraud Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/karthikamuthiah/fraud-detection.b96554ec-5a25-404e-ad6e-6522c652146a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250425/auto/storage/goog4_request%26X-Goog-Date%3D20250425T173026Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3De788529a1924669a2825c1b2a893db5fc14cfe4cb87c06866a79cb7978f5a5e07e169fec5f84d92146b684a999d0085010381c636d780a0038ab6bfeb59813a63063dba8d003cd067fae977ba19cdcb903aeff55d49e81be4f4454a7799160821fbb374a59205200b79c3ebbb3cbc5209d61ef17f92affca8ff08e86004eeea5cba10b57065a12411bd0b1ddd23db440856673b3ca18526f2d4c02c7cfd23c4d18bf726b5814a187f53fe25fcfa3aa443ece16c06ef2980e14c6cb2b6cae9a4c7c8c0c5536a905de5bf5848574e296d4089553637385e85971a040ffe81f838762e28ff176d0b5142710a05e7c167f713a87b5de20293140964a2f68b5e6a9a7
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
karthikamuthiah_creditcard_2023_path = kagglehub.dataset_download('karthikamuthiah/creditcard-2023')

print('Data source import complete.')

"""# Data Overview

## Understanding the Data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('/kaggle/input/creditcard-2023/creditcard_2023.csv')

print(df.info())

print(df.describe(include='all'))

import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style
sns.set(style="whitegrid")

# Loop through columns
for column in df.columns:
    plt.figure(figsize=(8, 4))

    if pd.api.types.is_numeric_dtype(df[column]):
        #Numeric columns
        # Filter out extreme outliers for better visualization
        data = df[column].dropna()
        q_low = data.quantile(0.01)
        q_high = data.quantile(0.99)
        trimmed_data = data[(data >= q_low) & (data <= q_high)]

        sns.histplot(trimmed_data, bins=30, kde=True)
        plt.title(f'Zoomed Frequency Distribution: {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
    else:
        # Categorical columns
        value_counts = df[column].value_counts(dropna=False)
        sns.barplot(x=value_counts.index.astype(str), y=value_counts.values)
        plt.title(f'Frequency Distribution: {column}')
        plt.xlabel(column)
        plt.ylabel('Frequency')
        plt.xticks(rotation=45)

    plt.tight_layout()
    plt.show()

#Boxplot to look for outliers

numeric_cols = df.select_dtypes(include='number').columns

for col in numeric_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col], orient='h')
    plt.title(f'Box Plot for: {col}')
    plt.xlabel(col)
    plt.tight_layout()
    plt.show()

"""## Findings

* This dataset contains 30 variables, an ID column, 28 anonymized features(v1-v28), Transaction amount and Class.
* Signs of Missingness : count of variables is not consistent across the columns (1 lesser in some columns).
* The features seems to be standardized (Mean is close to 0 and Standard Deviation is close to 1) and mostly normalized based on the frequency distribution. However,that will need to be confirmed using appropriate statistical tests.
* Outliers in box plots might be a result of Standardization as they are defined by the standard 1.5 × IQR method.Unable to tell if they are actual anomalies as we do not know what they measure. Potential way to address would be to check how many of them have a z-score > 3 as most models are robust to z-score > 3.

# Data Preprocessing

## Missing Values
"""

# Get rows with any missing values
rows_with_missing = df[df.isnull().any(axis=1)]

print(rows_with_missing)

"""* Target Variable unavailable, hence row with missing data is removed from dataset."""

#drop rows with missing data
df_cleaned = df.dropna()

# Check for missing values
missing_values = df_cleaned.isnull().sum()
missing_columns = missing_values[missing_values > 0]

print("Missing values in each column:")
print(missing_columns)

"""## Irrelevant Data

* ID column to be discarded.
* All other feature variables to be kept as we are unable to confirm what they measure.
"""

df_cleaned = df_cleaned.drop(columns=['id'])

"""## Feature Engineering

* Though highly relevant in the case of Fraud Detection, we are unable to create new features as all the columns are anonymized.
* The only column with raw data is transaction amount, which can be used to create new features like ratio of transaction amount/user's average transaction amount but due to lack of contextual information on the variables, we are unable to do so.

## Outliers
"""

df_featuresonly = df_cleaned.drop(columns=["Amount", "Class"])

import numpy as np
from scipy.stats import zscore

z_scores = df_featuresonly.apply(zscore)
outliers = (np.abs(z_scores) > 3)

# Count of outliers per column
outliers_per_column = outliers.sum()
outlier_percentage = (outliers_per_column/562540)*100
print(outlier_percentage)

rows_with_outliers = df_featuresonly[(np.abs(z_scores) > 3).any(axis=1)]

print("Rows with outliers based on z-score > 3:")
print(rows_with_outliers)

"""* After increasing the z-score threshold to 3, the percentage of outliers in each column range from 0.002% - 2.1% . Perfectly reasonable for real-world data.
* Rows with outliers form about 9.1% of the total dataset.

Since the percentage of outliers in each column is within the acceptable range, there is no need to winsorize the data, impute or remove the outliers.

## Standardizing Variables

* Feature variables are already standardized.
* "Amount" variable will be standardized as we will be running Logistic Regression, which is a model sensitive to feature scales.
* For other tree-based models RandomForest, XGBoost that are not affected by feature scales, we will be using the raw "Amount" variable.
"""

from sklearn.preprocessing import StandardScaler

df_cleaned['Amount_raw'] = df_cleaned['Amount']
scaler = StandardScaler()
df_cleaned['Amount'] = scaler.fit_transform(df_cleaned[['Amount']])

df_cleaned.head(10)

"""# Assumption Checklist for Logistic Regression

1. Response variable is binary ✅
 * Yes. The "Class" variable only has 2 responses 0 or 1 indicating cases of No Fraud and Fraud respectively.
2. Observations are independent (Assumed to be ✅)
 * Cannot be verified with existing information.
 * If we had card details, we could have checked if there were multiple fraudulant transactions on the same card to confirm if our data is clustered.
 * If we had time data, we could have checked for any temporal auto-correlation.
 * Hence we assume this as a limitation of this model and proceed by considering this assumption fulfilled.
"""

#Checking for multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# X: your feature matrix excluding target variable raw Amount
X = add_constant(df_cleaned.drop(columns=['Class', 'Amount_raw']))

vif_df = pd.DataFrame()
vif_df["Variable"] = X.columns
vif_df["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_df)

"""3. No multicollinearity among explanatory variables ✅
 * Variable Inflation Factor is below the threshold of 5 for all variables except "V17" (VIF = 5.26). Still close to threshold, so need not be removed at this stage, but something to pay attention to when we have the regression co-efficients.
4. No extreme outliers ✅
 * Negligible percentage of outliers in affected columns.
"""

#Box-Tidwell Test
import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy.special import logit
from statsmodels.discrete.discrete_model import Logit

# Copy to avoid modifying the original data
df_temp = df_cleaned.copy()

# Drop any rows with NaNs
df_temp = df_temp.replace([np.inf, -np.inf], np.nan).dropna()

# Exclude target and irrelevant columns
exclude_cols = ['Class', 'Amount_raw']
continuous_vars = [col for col in df_temp.columns if col not in exclude_cols]

# Box-Tidwell
# Add a constant (like 1.1) to shift values if needed
for col in continuous_vars:
    min_val = df_temp[col].min()
    if min_val <= 0:
        df_temp[col] = df_temp[col] + abs(min_val) + 1.1

# Add interaction terms (X * log(X)) for each continuous predictor
for col in continuous_vars:
    df_temp[f"{col}_logint"] = df_temp[col] * np.log(df_temp[col])

# Define X and y
X = df_temp[continuous_vars + [f"{col}_logint" for col in continuous_vars]]
X = sm.add_constant(X)
y = df_temp['Class']

# Fit logistic regression
model = Logit(y, X)
result = model.fit()

# Display summary
print(result.summary())

"""5. Linear relationship between feature variables and the logit of the target variable ❎
 * As evidenced by the Box-Tidwell Test, all feature variables do NOT have a linear relationship with the logit of the target variable (p-value<0.05).
 * Logistic Regression therefore, might not deliver the best model for this data. However, it could serve as a baseline for the other models.


"""

target_col = 'Class'
n_rows = len(df_cleaned)
event_rate = np.mean(df_cleaned[target_col])  # proportion of 1s
n_predictors = df_cleaned.drop(columns=target_col).shape[1]

required_n = (10 * n_predictors) / event_rate
print(f"Required sample size (EPV=10): {required_n:.0f}")
print(f"Actual sample size: {n_rows}")

"""6. Sample Size is sufficiently large ✅
 * A commonly cited rule of thumb is to have at least 10 events per predictor to avoid overfitting and ensure model stability.The actual sample size as evidenced above is way above the required sample size.

## Steps to address Non-Linearity between features and logit of target variable

Check the relationship visually
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Replace this with your actual target column name
target = 'Class'

# Loop through all columns except the target
for col in df_cleaned.columns:
    if col == target:
        continue

    try:
        # Check if column is numeric
        if pd.api.types.is_numeric_dtype(df_cleaned[col]):
            plt.figure(figsize=(6, 4))
            sns.regplot(
                x=col,
                y=target,
                data=df_cleaned,
                logistic=True,
                ci=None,
                scatter_kws={'alpha': 0.3}
            )
            plt.title(f"Logistic Fit: {col} vs {target}")
            plt.xlabel(col)
            plt.ylabel(target)
            plt.tight_layout()
            plt.show()
    except Exception as e:
        print(f"Could not plot {col}: {e}")

import numpy as np
import pandas as pd
import statsmodels.api as sm
from statsmodels.discrete.discrete_model import Logit

# Start with your cleaned and positive-shifted dataset
df_temp = df_cleaned.copy()
df_temp = df_temp.replace([np.inf, -np.inf], np.nan).dropna()

# Target and continuous vars
exclude_cols = ['Class', 'Amount_raw']
continuous_vars = [col for col in df_temp.columns if col not in exclude_cols]
target = 'Class'

# Shift variables to be positive
for col in continuous_vars:
    min_val = df_temp[col].min()
    if min_val <= 0:
        df_temp[col] = df_temp[col] + abs(min_val) + 1.1

# Store results
bt_results = []

# Run Box-Tidwell for log and sqrt transformations of each variable
for col in continuous_vars:
    print(f"\n🔍 Evaluating transformations for: {col}")

    for transform in ['log', 'sqrt']:
        if transform == 'log':
            df_temp[f"{col}_{transform}"] = np.log(df_temp[col] + 1e-6)
        else:  # sqrt
            df_temp[f"{col}_{transform}"] = np.sqrt(df_temp[col])

        # Interaction term: x * log(x)
        inter_term = f"{col}_{transform}_logint"
        df_temp[inter_term] = df_temp[f"{col}_{transform}"] * np.log(df_temp[f"{col}_{transform}"] + 1e-6)

        # Prepare X and y
        X = df_temp[[f"{col}_{transform}", inter_term]]
        X = sm.add_constant(X)
        y = df_temp[target]

        try:
            model = Logit(y, X)
            result = model.fit(disp=0)
            p_value = result.pvalues[inter_term]

            bt_results.append({
                'Variable': col,
                'Transformation': transform,
                'Interaction Term': inter_term,
                'P-value': p_value,
                'Passes Linearity': p_value >= 0.05
            })

            print(f"  {transform} → p = {p_value:.4f} → {'✅ Passes' if p_value >= 0.05 else '❌ Fails'}")

        except Exception as e:
            print(f"  {transform} → Model failed: {e}")

# Compile results
bt_df = pd.DataFrame(bt_results)

"""* Only the "Amount" variable has passed the Box-Tidwell test after log & sqrt transformation. We will use the log transformed "Amount" for the final model as it has a higher p-value.


"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
from patsy import dmatrix
from statsmodels.discrete.discrete_model import Logit

df_temp = df_cleaned.copy()
exclude_cols = ['Class', 'Amount_raw','Amount']
continuous_vars = [col for col in df_temp.columns if col not in exclude_cols]

spline_results = []

for col in continuous_vars:
    try:
        print(f"\n🔍 Applying spline transformation for: {col}")

        # Spline with fewer degrees and drop intercept to reduce collinearity
        spline = dmatrix(f"bs(df_temp['{col}'], df=3, degree=3, include_intercept=False)",
                         {"df_temp['{col}']": df_temp[col]}, return_type='dataframe')

        # Prepare X and y
        X = sm.add_constant(spline)
        y = df_temp['Class']

        model = Logit(y, X)
        result = model.fit(disp=0)

        # Just check the max p-value in the spline terms
        p_value = result.pvalues[1:].max()  # Skip the constant

        spline_results.append({
            'Variable': col,
            'Max_P_value_in_spline_terms': p_value
        })

        print(f"  {col} spline → max p = {p_value:.4f}")

    except Exception as e:
        print(f"  ⚠️ Error for {col}: {e}")

# Compile all results
spline_results_df = pd.DataFrame(spline_results)

"""* Only the variable "V4 & 24" has passed the non-linearity assumption.(V25 is marginal)

# Class Imbalance Check
"""

class_counts = df_cleaned['Class'].value_counts()
print(class_counts)

# Calculate proportion
class_ratio = class_counts.min() / class_counts.max()
print(f"Imbalance Ratio: {class_ratio:.2f}")

"""* No Class Imbalance observed, SMOTE not required

# Logistic Regression Models

We will be running 3 different models and compare the model metrics.
*   Model 1 : With log transformed "Amount" & spline transformed V4 & V24
*   Model 2 : With log transformed "Amount" & spline transformed V4 & V24 & V25
*   Model 3 : With log transformed "Amount" & all other feature variables(v1-28) regularized (no transformation required)

## Model 1
"""

X = df_cleaned[['Amount','V4', 'V24']]
has_nan = X.isnull().values.any()

print("Contains NaN values:", has_nan)

X.head()
print(X.isna().sum())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import FunctionTransformer, SplineTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

#Define terms
X = df_cleaned[['Amount','V4', 'V24']].copy()
y = df_cleaned['Class']

# Handle negatives and missing values
X['Amount'] = X['Amount'].clip(lower=0)
X['Amount'].fillna(X['Amount'].median(), inplace=True)
X['V4'].fillna(X['V4'].median(), inplace=True)
X['V24'].fillna(X['V24'].median(), inplace=True)


#Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

#Transform data
# Log-transform 'Amount'
log_transformer = FunctionTransformer(np.log1p, validate=True)
X_train_amt_log = log_transformer.fit_transform(X_train[['Amount']])
X_test_amt_log = log_transformer.transform(X_test[['Amount']])

# Spline-transform 'V4&24'
spline_transformer = SplineTransformer(degree=3, n_knots=4, include_bias=False)
X_train_v_spline = spline_transformer.fit_transform(X_train[['V4','V24']])
X_test_v_spline = spline_transformer.transform(X_test[['V4','V24']])

# Combine transformed features
X_train_transformed = np.hstack((X_train_amt_log, X_train_v_spline))
X_test_transformed = np.hstack((X_test_amt_log, X_test_v_spline))

# Optional sanity check
assert np.all(np.isfinite(X_train_transformed)), "NaNs or infs in X_train"
assert np.all(np.isfinite(X_test_transformed)), "NaNs or infs in X_test"

#Model Training
model = LogisticRegression(max_iter=1000)
model.fit(X_train_transformed, y_train)

# Predictions
y_pred = model.predict(X_test_transformed)
y_proba = model.predict_proba(X_test_transformed)[:, 1]

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba) if len(set(y)) == 2 else None
cm = confusion_matrix(y_test, y_pred)

# Print
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""## Model 2"""

#Define terms
X = df_cleaned[['Amount', 'V4', 'V24','V25']].copy()
y = df_cleaned['Class']

# Handle negatives and missing values
X['Amount'] = X['Amount'].clip(lower=0)
X['Amount'].fillna(X['Amount'].median(), inplace=True)
X['V4'].fillna(X['V4'].median(), inplace=True)
X['V24'].fillna(X['V24'].median(), inplace=True)
X['V25'].fillna(X['V25'].median(), inplace=True)

#Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

#Transform data (detailed for reference)
# Log-transform 'Amount'
log_transformer = FunctionTransformer(np.log1p, validate=True)
X_train_amt_log = log_transformer.fit_transform(X_train[['Amount']])
X_test_amt_log = log_transformer.transform(X_test[['Amount']])

# Spline-transform 'V4'
spline_transformer = SplineTransformer(degree=3, n_knots=4, include_bias=False)
X_train_v4_spline = spline_transformer.fit_transform(X_train[['V4']])
X_test_v4_spline = spline_transformer.transform(X_test[['V4']])

# Spline-transform 'V24'
X_train_v24_spline = spline_transformer.fit_transform(X_train[['V24']])
X_test_v24_spline = spline_transformer.transform(X_test[['V24']])

# Spline-transform 'V25'
X_train_v25_spline = spline_transformer.fit_transform(X_train[['V25']])
X_test_v25_spline = spline_transformer.transform(X_test[['V25']])

# Combine transformed features
X_train_transformed = np.hstack((X_train_amt_log, X_train_v4_spline, X_train_v24_spline, X_train_v25_spline))
X_test_transformed = np.hstack((X_test_amt_log, X_test_v4_spline, X_test_v24_spline, X_test_v25_spline))

# Optional sanity check
assert np.all(np.isfinite(X_train_transformed)), "NaNs or infs in X_train"
assert np.all(np.isfinite(X_test_transformed)), "NaNs or infs in X_test"

#Model Training
model = LogisticRegression(max_iter=1000)
model.fit(X_train_transformed, y_train)

#Predictions
y_pred = model.predict(X_test_transformed)
y_proba = model.predict_proba(X_test_transformed)[:, 1]

#Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba) if len(set(y)) == 2 else None
cm = confusion_matrix(y_test, y_pred)

#Print
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""## Model 3"""

#Define data
X = df_cleaned.drop(columns=['Class']).copy()
y = df_cleaned['Class']

# Handle missing values
X['Amount'].fillna(X['Amount'].median(), inplace=True)  # For Amount column
X.fillna(X.median(), inplace=True)  # For other columns

# Ensure no negative values in 'Amount' before applying log transformation
X['Amount'] = X['Amount'].clip(lower=0)

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Define columns
amount_col = ['Amount']
other_cols = [col for col in X.columns if col != 'Amount']

#Transform Data
# Log-transform 'Amount' (log1p handles 0 or positive values)
log_transformer = FunctionTransformer(np.log1p, validate=True)
X_train_amt_log = log_transformer.fit_transform(X_train[amount_col])
X_test_amt_log = log_transformer.transform(X_test[amount_col])

# Scale columns
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[other_cols])
X_test_scaled = scaler.transform(X_test[other_cols])

# Combine transformed features
X_train_transformed = np.hstack((X_train_amt_log, X_train_scaled))
X_test_transformed = np.hstack((X_test_amt_log, X_test_scaled))

# Ridge regularization
model = LogisticRegression(penalty='l2', max_iter=1000, solver='lbfgs')
model.fit(X_train_transformed, y_train)

#Prediction
y_pred = model.predict(X_test_transformed)
y_proba = model.predict_proba(X_test_transformed)[:, 1]

#Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba) if len(set(y)) == 2 else None
cm = confusion_matrix(y_test, y_pred)

#Print
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Define data
X = df_cleaned.drop(columns=['Class','Amount'])
y = df_cleaned['Class']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize the model
rf_model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    n_jobs=-1
)

# Fit the model
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)
y_proba = rf_model.predict_proba(X_test)[:, 1]

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# For ROC AUC, check if binary classification
roc_auc = None
if len(set(y)) == 2:
    roc_auc = roc_auc_score(y_test, y_proba)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Print
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""# XGBoost"""

import xgboost as xgb

# Define data
X = df_cleaned.drop(columns=['Class', 'Amount'])
y = df_cleaned['Class']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Initialize and fit XGBoost classifier
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'  # to suppress warning
)

xgb_model.fit(X_train, y_train)

# Predictions
y_pred = xgb_model.predict(X_test)
y_proba = xgb_model.predict_proba(X_test)[:, 1]

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba) if len(set(y)) == 2 else None
cm = confusion_matrix(y_test, y_pred)

# Print
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""# Neural Networks"""

from sklearn.neural_network import MLPClassifier

from sklearn.preprocessing import StandardScaler

# Define data
X = df_cleaned.drop(columns=['Class', 'Amount'])
y = df_cleaned['Class']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Initialize and fit the MLPClassifier
mlp_model = MLPClassifier(
    hidden_layer_sizes=(100,),  # one hidden layer with 100 neurons
    activation='relu',
    solver='adam',
    max_iter=300,
    random_state=42
)

mlp_model.fit(X_train, y_train)

# Predictions
y_pred = mlp_model.predict(X_test)
y_proba = mlp_model.predict_proba(X_test)[:, 1]

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba) if len(set(y)) == 2 else None
cm = confusion_matrix(y_test, y_pred)

# Print
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""# Linear Support Vector Machine (SVM)"""

from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Drop 'Class' and 'Amount' from features
X = df_cleaned.drop(columns=['Class', 'Amount'])
y = df_cleaned['Class']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# Initialize and fit the Linear SVM
svm_model = LinearSVC(
    max_iter=10000,
    random_state=42
)
svm_model.fit(X_train, y_train)

# Predictions
y_pred = svm_model.predict(X_test)

# Since LinearSVC doesn't support predict_proba, use decision_function
y_scores = svm_model.decision_function(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_scores) if len(set(y)) == 2 else None
cm = confusion_matrix(y_test, y_pred)

# Print results
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}" if roc_auc is not None else "ROC AUC: Not applicable")
print("Confusion Matrix:")
print(cm)

"""# Summary & Overview of Models

1. Logistic Regression Model 1
* Metrics: Accuracy: 0.8907, Precision: 0.8944, Recall: 0.8864, F1 Score: 0.8904, ROC AUC Score: 0.9548
* Strengths: Balanced performance across precision and recall, resulting in a solid F1 score. High ROC AUC suggests good ability to distinguish between classes.
* Weaknesses: Metrics are solid but not outstanding, leaving room for improvement in both precision and recall.
* Conclusion: A decent baseline model with reliable, balanced performance—suitable for scenarios where simplicity and interpretability are important.

2. Logistic Regression Model 2
* Metrics: Accuracy: 0.8922, Precision: 0.8965, Recall: 0.8873, F1 Score: 0.8918, ROC AUC Score: 0.9550
* Strengths: Slight improvements across most metrics compared to Model 1, with a better balance between precision and recall.
* Weaknesses: Similar to Model 1, the performance is consistent but not exceptional compared to more complex models.
* Conclusion: A small step up from Model 1, offering marginal gains. Still a stable and interpretable option, though not the most competitive overall.

3. Logistic Regression Model 3:
* Metrics: Accuracy: 0.9653, Precision: 0.9774, Recall: 0.9528, F1 Score: 0.9649, ROC AUC Score: 0.9934
* Strengths: Very strong across all metrics, particularly in precision, recall, and F1 score. The ROC AUC indicates excellent discriminatory power.
* Weaknesses: While strong, it may still fall short compared to more complex models like Random Forest in terms of raw accuracy.
* Conclusion: Among the logistic regression models, this is clearly the best performer. A solid candidate overall, although outperformed by ensemble models in high-accuracy use cases.

4. Random Forest:
* Metrics: Accuracy: 0.9999, Precision: 0.9998, Recall: 1.0000, F1 Score: 0.9999, ROC AUC Score: 1.0000
* Strengths: Perfect precision (0.9998), recall (1.0000), F1 score (0.9999), and ROC AUC score (1.0000). Achieves near-perfect results without any misclassifications.
* Weaknesses: None—this model is flawless in terms of performance metrics.
* Conclusion: The best-performing model based on all metrics. It achieves perfect precision and recall, making it the top choice in terms of overall performance.

5. XGBoost:
* Metrics: Accuracy: 0.9943, Precision: 0.9951, Recall: 0.9934, F1 Score: 0.9943, ROC AUC Score: 0.9998
* Strengths: High precision (0.9951), recall (0.9934), and F1 score (0.9943), with a very high ROC AUC score (0.9998).
* Weaknesses: While this model is excellent, it doesn't quite match the perfection of Random Forest in terms of recall and precision.
* Conclusion: A highly efficient and strong model, though slightly less than Random Forest in terms of performance. It’s a great choice but not the absolute best.

6. Neural Networks:
* Metrics: Accuracy: 0.9997, Precision: 0.9994, Recall: 1.0000, F1 Score: 0.9997, ROC AUC Score: 1.0000
* Strengths: Near-perfect precision (0.9994), recall (1.0000), F1 score (0.9997), and ROC AUC score (1.0000). Flawless results, similar to Random Forest.
* Weaknesses: Neural networks are more computationally intensive and require more memory, especially for deep architectures, making them less resource-efficient than Random Forest.
* Conclusion: Excellent performance, but neural networks are resource-heavy and typically require more powerful hardware for training.

7. Linear Support Vector Machine (SVM):
* Metrics: Accuracy: 0.9636, Precision: 0.9807, Recall: 0.9461, F1 Score: 0.9631, ROC AUC Score: 0.9932
* Strengths: High precision (0.9807) and recall (0.9461), with a good F1 score (0.9631).
* Weaknesses: Slightly lower recall than other top models (e.g., Random Forest, Neural Networks, and XGBoost).
* Conclusion: Strong performance but slightly less effective than other models in terms of recall and overall performance.

# Conclusion

Random Forest and Neural Networks both deliver excellent performance with near-perfect results across precision, recall, F1 score, and ROC AUC score. They are both the best-performing models in terms of accuracy and balance. However, when factoring in resource efficiency:

* Random Forest is more resource-efficient. It requires less computational power and memory than Neural Networks, making it a better choice if you're working with limited hardware or need faster training times. Additionally, Random Forest is easier to interpret, which can be helpful in understanding the model’s decision-making process.

* Neural Networks may provide slightly better accuracy in some cases but are resource-heavy, requiring more memory and processing power, especially for deep architectures. Therefore, unless you have access to specialized hardware (e.g., GPUs or TPUs) and need the absolute highest accuracy for complex tasks, Random Forest is the more efficient choice.

Thus, while Random Forest and Neural Networks are both top-tier models, Random Forest stands out as the more practical and resource-efficient option for most use cases.

# Feature Selection

## Feature Importance
"""

#Get Feature Importances
importances = rf_model.feature_importances_
feature_importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

#Print Feature Importances
print(feature_importance_df)

#Plot Feature Importances
plt.figure(figsize=(12, len(feature_importance_df) * 0.3))  # Adjust plot height based on number of features
sns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='viridis')
plt.title("All Feature Importances - Random Forest")
plt.tight_layout()
plt.show()

# Get top 20 features
top_20_features = feature_importance_df['Feature'].head(20).tolist()

# Filter Train and Test Sets
X_train_top20 = X_train[top_20_features]
X_test_top20 = X_test[top_20_features]

# Train RandomForest on Top 20 Features
rf_top20 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_top20.fit(X_train_top20, y_train)

# Predict and Evaluate
y_pred = rf_top20.predict(X_test_top20)
y_proba = rf_top20.predict_proba(X_test_top20)[:, 1]

# Evaluation Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
cm = confusion_matrix(y_test, y_pred)

# Print Evaluation
print("Evaluation for Random Forest with Top 20 Features")
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

"""* The metrics are the same as when all 28+Amount features were included in the model."""

# Get Top 10 Features
top_10_features = feature_importance_df['Feature'].head(10).tolist()

# Filter Train and Test Sets
X_train_top10 = X_train[top_10_features]
X_test_top10 = X_test[top_10_features]

# Train RandomForest on Top 10 Features
rf_top10 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_top10.fit(X_train_top10, y_train)

# Predict and Evaluate
y_pred = rf_top10.predict(X_test_top10)
y_proba = rf_top10.predict_proba(X_test_top10)[:, 1]

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
cm = confusion_matrix(y_test, y_pred)

# Print Evaluation
print("Evaluation for Random Forest with Top 10 Features")
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

"""* The metrics show that the model with thw Top 20 features fared better than the model with the Top 10 features only. Let us to retrain the model with the Top 15 variables to see exactly when the model looses it's efficiency."""

# Get Top 15 Features
top_15_features = feature_importance_df['Feature'].head(15).tolist()

# Filter Train and Test Sets
X_train_top15 = X_train[top_15_features]
X_test_top15 = X_test[top_15_features]

# Train RandomForest on Top 15 Features
rf_top15 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_top15.fit(X_train_top15, y_train)

# Predict and Evaluate
y_pred = rf_top15.predict(X_test_top15)
y_proba = rf_top15.predict_proba(X_test_top15)[:, 1]

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
cm = confusion_matrix(y_test, y_pred)

# Print Evaluation
print("Evaluation for Random Forest with Top 15 Features")
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

"""* The metrics are the same but one extra record has been classified wrongly. Let's try with the top 12 features next."""

# Get Top 12 Features
top_12_features = feature_importance_df['Feature'].head(12).tolist()

# Filter Train and Test Sets
X_train_top12 = X_train[top_12_features]
X_test_top12 = X_test[top_12_features]

# Train RandomForest on Top 12 Features
rf_top12 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_top12.fit(X_train_top12, y_train)

# Predict and Evaluate
y_pred = rf_top12.predict(X_test_top12)
y_proba = rf_top12.predict_proba(X_test_top12)[:, 1]

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
cm = confusion_matrix(y_test, y_pred)

# Print Evaluation
print("Evaluation for Random Forest with Top 12 Features")
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

"""* Model performance has dropped significantly. We will stick to 13 features, if it can uphold the same metrics."""

# Get Top 13 Features
top_13_features = feature_importance_df['Feature'].head(13).tolist()

# Filter Train and Test Sets
X_train_top13 = X_train[top_13_features]
X_test_top13 = X_test[top_13_features]

# Train Random Forest on Top 13 Features
rf_top13 = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_top13.fit(X_train_top13, y_train)

# Predict and Evaluate
y_pred = rf_top13.predict(X_test_top13)
y_proba = rf_top13.predict_proba(X_test_top13)[:, 1]

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
cm = confusion_matrix(y_test, y_pred)

# Print Evaluation
print("Evaluation for Random Forest with Top 13 Features")
print(f"Accuracy       : {accuracy:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"F1 Score       : {f1:.4f}")
print(f"ROC AUC Score  : {roc_auc:.4f}")
print("Confusion Matrix:")
print(cm)

"""Therefore, using the top 13 variables would produce an equally accurate but more resource efficient model.

# Hyperparameter tuning

(Pipeline to try hyperparameter tuning)
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from scipy.stats import randint

# Top 13 Features
top_13_features = feature_importance_df['Feature'].head(13).tolist()
X_train_top13 = X_train[top_13_features]
X_test_top13 = X_test[top_13_features]

# Define the parameter grid for RandomizedSearch
param_dist = {
    'n_estimators': randint(50, 300),
    'max_depth': [None] + list(range(5, 30, 5)),
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]
}

# Initialize Random Forest Classifier
rf = RandomForestClassifier(random_state=42)

# Setup RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=50,  # Increase for better results
    scoring='f1',
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# Fit the randomized search
random_search.fit(X_train_top13, y_train)

# Best model
best_rf = random_search.best_estimator_

# Predict with the best model
y_pred = best_rf.predict(X_test_top13)
y_proba = best_rf.predict_proba(X_test_top13)[:, 1]

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
cm = confusion_matrix(y_test, y_pred)

# Results
print("Evaluation for Tuned Random Forest with Top 13 Features")
print(f"Best Parameters  : {random_search.best_params_}")
print(f"Accuracy         : {accuracy:.4f}")
print(f"Precision        : {precision:.4f}")
print(f"Recall           : {recall:.4f}")
print(f"F1 Score         : {f1:.4f}")
print(f"ROC AUC Score    : {roc_auc:.4f}")
print("Confusion Matrix :")
print(cm)

"""# Conclusion

This fraud detection project involved testing multiple machine learning models, with the Random Forest classifier emerging as the best performer. Surprisingly, the model remained equally effective when limited to the top 13 features, all derived from PCA, indicating strong dimensionality reduction. Transaction amount, the only raw variable, was the least predictive—possibly because it varies widely between individuals and lacks standardized context. Although a hyperparameter tuning pipeline was developed to further improve model performance, long runtimes on Kaggle led to postponing its implementation. The pipeline will be tested on a smaller dataset in future projects.
"""